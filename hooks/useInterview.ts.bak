'use client';

import { useState, useRef, useCallback, useEffect } from 'react';
import { Room, RoomEvent, Track, RemoteTrack, RemoteAudioTrack } from 'livekit-client';
import { createClient, LiveTranscriptionEvents } from '@deepgram/sdk';

interface ConversationMessage {
  role: 'user' | 'assistant';
  content: string;
}

export function useInterview() {
  const [isInterviewActive, setIsInterviewActive] = useState(false);
  const [isMicActive, setIsMicActive] = useState(false);
  const [isAISpeaking, setIsAISpeaking] = useState(false);
  const [conversationHistory, setConversationHistory] = useState<ConversationMessage[]>([]);
  const [currentTranscript, setCurrentTranscript] = useState('');
  const [audioLevel, setAudioLevel] = useState(0);

  const roomRef = useRef<Room | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const deepgramSocketRef = useRef<WebSocket | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const accumulatedTranscriptRef = useRef<string>('');
  const isProcessingRef = useRef(false);

  // Audio level monitoring with silence detection
  const monitorAudioLevel = useCallback(() => {
    if (!analyserRef.current) return;

    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    let animationId: number;
    
    const checkLevel = () => {
      if (!analyserRef.current) return;
      
      analyserRef.current.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
      const normalizedLevel = average / 255; // Normalize to 0-1
      setAudioLevel(normalizedLevel);
      
      // Detect silence (level below threshold for turn detection)
      if (isMicActive && normalizedLevel < 0.05) {
        // User might have stopped speaking
        if (!silenceDetectionTimerRef.current) {
          silenceDetectionTimerRef.current = setTimeout(() => {
            // After 1.5 seconds of silence, finalize the transcript
            if (accumulatedTranscriptRef.current.trim()) {
              finalizeTranscript();
            }
          }, 1500);
        }
      } else if (isMicActive && normalizedLevel >= 0.05) {
        // User is speaking, clear silence timer
        if (silenceDetectionTimerRef.current) {
          clearTimeout(silenceDetectionTimerRef.current);
          silenceDetectionTimerRef.current = null;
        }
      }
      
      if (isMicActive || isAISpeaking) {
        animationId = requestAnimationFrame(checkLevel);
      }
    };
    
    checkLevel();
    
    return () => {
      if (animationId) {
        cancelAnimationFrame(animationId);
      }
    };
  }, [isMicActive, isAISpeaking]);

  // Start the interview
  const startInterview = useCallback(async () => {
    setIsInterviewActive(true);
    
    // Greet the user with Deepgram TTS
    const greeting = "Hello! Welcome to your interview session. I'm your AI interviewer today. Let's begin with a simple question: Can you tell me about yourself and your background?";
    
    setConversationHistory([{ role: 'assistant', content: greeting }]);
    
    // Play greeting via Deepgram TTS
    await playTextToSpeech(greeting);
  }, []);

  // Play text using Deepgram TTS
  const playTextToSpeech = async (text: string) => {
    try {
      setIsAISpeaking(true);
      
      const response = await fetch('/api/tts', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text }),
      });

      if (!response.ok) throw new Error('TTS request failed');

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);

      // Monitor audio playback for visualization
      if (!audioContextRef.current) {
        audioContextRef.current = new AudioContext();
      }

      const source = audioContextRef.current.createMediaElementSource(audio);
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 256;
      
      source.connect(analyserRef.current);
      analyserRef.current.connect(audioContextRef.current.destination);

      monitorAudioLevel();

      audio.onended = () => {
        setIsAISpeaking(false);
        setAudioLevel(0);
        URL.revokeObjectURL(audioUrl);
      };

      await audio.play();
    } catch (error) {
      console.error('TTS Error:', error);
      setIsAISpeaking(false);
    }
  };

  // Toggle microphone
  const toggleMicrophone = useCallback(async () => {
    if (!isMicActive) {
      // Start microphone
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaStreamRef.current = stream;
        setIsMicActive(true);

        // Setup audio analysis for visualization
        if (!audioContextRef.current) {
          audioContextRef.current = new AudioContext();
        }
        
        const source = audioContextRef.current.createMediaStreamSource(stream);
        analyserRef.current = audioContextRef.current.createAnalyser();
        analyserRef.current.fftSize = 256;
        source.connect(analyserRef.current);

        monitorAudioLevel();

        // Connect to Deepgram for STT via LiveKit or direct WebSocket
        await startDeepgramSTT(stream);
        
      } catch (error) {
        console.error('Microphone access error:', error);
      }
    } else {
      // Stop microphone
      stopMicrophone();
    }
  }, [isMicActive, monitorAudioLevel]);

  // Start Deepgram STT with proper error handling and turn detection
  const startDeepgramSTT = async (stream: MediaStream) => {
    try {
      // Get Deepgram WebSocket connection
      const response = await fetch('/api/stt-token');
      const { token } = await response.json();

      // Enhanced Deepgram connection with noise suppression and smart formatting
      const wsUrl = new URL('wss://api.deepgram.com/v1/listen');
      wsUrl.searchParams.append('encoding', 'linear16');
      wsUrl.searchParams.append('sample_rate', '48000');
      wsUrl.searchParams.append('model', 'nova-2');
      wsUrl.searchParams.append('smart_format', 'true');
      wsUrl.searchParams.append('interim_results', 'true');
      wsUrl.searchParams.append('utterance_end_ms', '1500');
      wsUrl.searchParams.append('vad_events', 'true'); // Voice Activity Detection
      wsUrl.searchParams.append('punctuate', 'true');

      const socket = new WebSocket(wsUrl.toString(), ['token', token]);
      deepgramSocketRef.current = socket;

      socket.onopen = () => {
        console.log('‚úÖ Deepgram WebSocket connected with VAD');
        
        // Create MediaRecorder with optimized settings
        const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
          ? 'audio/webm;codecs=opus'
          : 'audio/webm';
        
        const mediaRecorder = new MediaRecorder(stream, {
          mimeType,
          audioBitsPerSecond: 128000,
        });
        
        mediaRecorderRef.current = mediaRecorder;

        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0 && socket.readyState === WebSocket.OPEN) {
            socket.send(event.data);
          }
        };

        mediaRecorder.onerror = (error) => {
          console.error('MediaRecorder error:', error);
        };

        // Start recording with smaller chunks for faster processing
        mediaRecorder.start(250);
        console.log('üé§ Recording started with 250ms chunks');
      };

      socket.onmessage = async (message) => {
        try {
          const data = JSON.parse(message.data);
          
          // Handle VAD events
          if (data.type === 'UtteranceEnd') {
            console.log('üîá Utterance ended - finalizing...');
            if (accumulatedTranscriptRef.current.trim()) {
              finalizeTranscript();
            }
            return;
          }
          
          // Handle transcription results
          if (data.channel?.alternatives?.[0]?.transcript) {
            const transcript = data.channel.alternatives[0].transcript;
            
            if (transcript.trim()) {
              console.log('üìù Transcript:', transcript, 'Final:', data.is_final);
              
              // Update current transcript for display
              setCurrentTranscript(transcript);
              
              // Accumulate transcript
              if (data.is_final) {
                accumulatedTranscriptRef.current += (accumulatedTranscriptRef.current ? ' ' : '') + transcript;
                console.log('‚úÖ Accumulated:', accumulatedTranscriptRef.current);
              }
            }
          }
        } catch (error) {
          console.error('Error parsing Deepgram message:', error);
        }
      };

      socket.onerror = (error) => {
        console.error('‚ùå Deepgram WebSocket error:', error);
      };

      socket.onclose = () => {
        console.log('üîå Deepgram WebSocket closed');
      };

    } catch (error) {
      console.error('‚ùå Deepgram STT setup error:', error);
    }
  };

  // Finalize transcript and send to AI
  const finalizeTranscript = async () => {
    const finalText = accumulatedTranscriptRef.current.trim();
    
    if (!finalText) return;
    
    console.log('üéØ Finalizing transcript:', finalText);
    
    // Clear accumulated transcript
    accumulatedTranscriptRef.current = '';
    
    // Clear current transcript display
    setCurrentTranscript('');
    
    // Stop microphone
    stopMicrophone();
    
    // Process the transcript
    await handleUserResponse(finalText);
  };

  // Handle user response and get AI reply
  const handleUserResponse = async (userText: string) => {
    console.log('üí¨ Processing user response:', userText);
    
    // Add user message to history
    const updatedHistory = [...conversationHistory, { role: 'user' as const, content: userText }];
    setConversationHistory(updatedHistory);
    
    console.log('üìä Updated conversation history:', updatedHistory);

    // Send to Gemini for processing
    try {
      console.log('ü§ñ Sending to Gemini API...');
      
      const response = await fetch('/api/interview', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ conversationHistory: updatedHistory }),
      });

      if (!response.ok) {
        throw new Error(`API responded with status: ${response.status}`);
      }

      const { reply, shouldEnd } = await response.json();
      
      console.log('‚úÖ Gemini response:', reply);

      // Add AI response to history
      const finalHistory = [...updatedHistory, { role: 'assistant' as const, content: reply }];
      setConversationHistory(finalHistory);

      // Play AI response
      await playTextToSpeech(reply);

      // Check if interview should end
      if (shouldEnd || reply.includes('Thank you for your time, we will get back to you soon')) {
        console.log('üëã Interview ending...');
        setIsInterviewActive(false);
        setIsMicActive(false);
      }

    } catch (error) {
      console.error('‚ùå Interview API error:', error);
      // Show error to user
      alert('Sorry, there was an error processing your response. Please try again.');
    }
  };

  // Stop microphone
  const stopMicrophone = () => {
    console.log('üõë Stopping microphone...');
    
    // Stop media recorder
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current = null;
    }
    
    // Stop media stream
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => {
        track.stop();
        console.log('‚èπÔ∏è Stopped track:', track.kind);
      });
      mediaStreamRef.current = null;
    }
    
    // Close WebSocket
    if (deepgramSocketRef.current) {
      if (deepgramSocketRef.current.readyState === WebSocket.OPEN) {
        deepgramSocketRef.current.close();
      }
      deepgramSocketRef.current = null;
    }
    
    // Clear silence detection timer
    if (silenceDetectionTimerRef.current) {
      clearTimeout(silenceDetectionTimerRef.current);
      silenceDetectionTimerRef.current = null;
    }
    
    setIsMicActive(false);
    setAudioLevel(0);
    setCurrentTranscript('');
  };

  // End interview
  const endInterview = useCallback(() => {
    console.log('üèÅ Ending interview...');
    stopMicrophone();
    setIsInterviewActive(false);
    setConversationHistory([]);
    setAudioLevel(0);
    accumulatedTranscriptRef.current = '';
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      console.log('üßπ Cleaning up...');
      stopMicrophone();
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      if (silenceDetectionTimerRef.current) {
        clearTimeout(silenceDetectionTimerRef.current);
      }
    };
  }, []);

  // Manual submit for current transcript
  const submitCurrentTranscript = useCallback(async () => {
    const textToSubmit = accumulatedTranscriptRef.current.trim() || currentTranscript.trim();
    
    if (!textToSubmit) {
      console.log('‚ö†Ô∏è No transcript to submit');
      return;
    }
    
    console.log('üì§ Manual submit:', textToSubmit);
    await finalizeTranscript();
  }, [currentTranscript]);

  return {
    isInterviewActive,
    isMicActive,
    isAISpeaking,
    conversationHistory,
    currentTranscript,
    audioLevel,
    startInterview,
    toggleMicrophone,
    endInterview,
    submitCurrentTranscript,
  };
}
